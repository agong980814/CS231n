{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, gym, time, glob, argparse, sys\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "from scipy.misc import imresize # preserves single-pixel info _unlike_ img = img[::2,::2]\n",
    "import tensorflow as tf\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# arguments\n",
    "env_name = 'Pong-v0'\n",
    "num_process = 20 # number of processes to train with\n",
    "render = False # whether to render the environment or not\n",
    "test = False # set lr=0, chooses most likely actions\n",
    "rnn_steps = 20 # steps to train LSTM\n",
    "lr = 1e-4\n",
    "seed = 1 # random seed for reproducibility\n",
    "gamma = 0.99 # reward discount factor\n",
    "tau = 1.0 # generalized adwantage estimation discount\n",
    "horizon = 0.99 # horizon for running averages\n",
    "h_size = 256 # hidden size \n",
    "num_actions = gym.make(env_name).action_space.n \n",
    "# get the action space of this game\n",
    "print(num_actions)\n",
    "env = gym.make(env_name)\n",
    "s = env.reset()\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate discounte rewards\n",
    "discount = lambda x, gamma: lfilter([1],[1,-gamma],x[::-1])[::-1] # discounted rewards one liner\n",
    "\n",
    "# preprocessing 210*160*3 to 80*80\n",
    "prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32).reshape(1,80,80)/255.\n",
    "\n",
    "def printlog(args, s, end='\\n', mode='a'):\n",
    "    print(s, end=end) ; f=open(args.save_dir+'log.txt',mode) ; f.write(s+'\\n') ; f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyNetwork(num_pixels, h_size): # an policy gardient network\n",
    "    # input batch_size * 6400\n",
    "    pixels = tf.placeholder(dtype=tf.float32, shape=(None, pixels_num))    \n",
    "    actions = tf.placeholder(dtype=tf.float32, shape=(None,1))\n",
    "    rewards = tf.placeholder(dtype=tf.float32, shape=(None,1))\n",
    "\n",
    "    with tf.variable_scope('policy'):\n",
    "    hidden = tf.layers.dense(pixels, hidden_units, activation=tf.nn.relu,\\\n",
    "            kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "    logits = tf.layers.dense(hidden, 1, activation=None,\\\n",
    "            kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    out = tf.sigmoid(logits, name=\"sigmoid\")\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=actions, logits=logits, name=\"cross_entropy\")\n",
    "    loss = tf.reduce_sum(tf.multiply(rewards, cross_entropy, name=\"rewards\"))\n",
    "\n",
    "    # lr=1e-4\n",
    "    lr=1e-3\n",
    "    decay_rate=0.99\n",
    "    opt = tf.train.RMSPropOptimizer(lr, decay=decay_rate).minimize(loss)\n",
    "    # opt = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "    tf.summary.histogram(\"hidden_out\", hidden)\n",
    "    tf.summary.histogram(\"logits_out\", logits)\n",
    "    tf.summary.histogram(\"prob_out\", out)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # grads = tf.gradients(loss, [hidden_w, logit_w])\n",
    "    # return pixels, actions, rewards, out, opt, merged, grads\n",
    "    return pixels, actions, rewards, out, opt, merged\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
