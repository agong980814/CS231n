{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, gym, time, glob, argparse, sys\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "from scipy.misc import imresize # preserves single-pixel info _unlike_ img = img[::2,::2]\n",
    "import threading\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--env', default='Breakout-v4', type=str, help='gym environment')\n",
    "    parser.add_argument('--processes', default=20, type=int, help='number of processes to train with')\n",
    "    parser.add_argument('--render', default=False, type=bool, help='renders the atari environment')\n",
    "    parser.add_argument('--test', default=False, type=bool, help='sets lr=0, chooses most likely actions')\n",
    "    parser.add_argument('--rnn_steps', default=20, type=int, help='steps to train LSTM over')\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--seed', default=1, type=int, help='seed random # generators (for reproducibility)')\n",
    "    parser.add_argument('--gamma', default=0.99, type=float, help='rewards discount factor')\n",
    "    parser.add_argument('--tau', default=1.0, type=float, help='generalized advantage estimation discount')\n",
    "    parser.add_argument('--horizon', default=0.99, type=float, help='horizon for running averages')\n",
    "    parser.add_argument('--hidden', default=256, type=int, help='hidden size of GRU')\n",
    "    return parser.parse_args()\n",
    "\n",
    "discount = lambda x, gamma: lfilter([1],[1,-gamma],x[::-1])[::-1] # discounted rewards one liner\n",
    "prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32).reshape(1,80,80)/255.\n",
    "\n",
    "def printlog(args, s, end='\\n', mode='a'):\n",
    "    print(s, end=end) ; f=open(args.save_dir+'log.txt',mode) ; f.write(s+'\\n') ; f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arguments\n",
    "env_name = 'Breakout-v4'\n",
    "num_process = 20 # number of processes to train with\n",
    "render = False # whether to render the environment or not\n",
    "test = False # set lr=0, chooses most likely actions\n",
    "rnn_steps = 20 # steps to train LSTM\n",
    "lr = 1e-4\n",
    "seed = 1 # random seed for reproducibility\n",
    "gamma = 0.99 # reward discount factor\n",
    "tau = 1.0 # generalized adwantage estimation discount\n",
    "horizon = 0.99 # horizon for running averages\n",
    "h_size = 256 # hidden size of Gated Recurrent Unit\n",
    "num_actions = gym.make(env_name).action_space.n \n",
    "# get the action space of this game\n",
    "env = gym.make(env_name)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = lambda x, gamma: lfilter([1],[1,-gamma],x[::-1])[::-1] # discounted rewards one liner\n",
    "prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32).reshape(1,80,80)/255.\n",
    "\n",
    "def printlog(args, s, end='\\n', mode='a'):\n",
    "    print(s, end=end) ; f=open(args.save_dir+'log.txt',mode) ; f.write(s+'\\n') ; f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C(): # an actor-critic neural network\n",
    "    def __init__(self, channels, memsize, num_actions):\n",
    "        sellf.inputs = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "        self.conv1 = tf.layers.conv2d(self.inputs, 32, 3, 2, padding='same')\n",
    "        self.conv2 = tf.layers.conv2d(self.conv1, 32, 3, 2, padding='same')       self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = tf.layers.conv2d(self.conv2, 32, 3, 2, padding='same')\n",
    "        self.conv4 = tf.layers.conv2d(self.conv3, 32, 3, 2, padding='same')\n",
    "        # convolution layers output size 32*5*5\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units=h_size)\n",
    "        init_state = cell.zero_sate(3, dtype=tf.float32)\n",
    "        self.gru = tf.nn.dymanic_rnn(cell, self.conv4, initial_sate=init_state, time_major=False)\n",
    "        self.critic = tf.layers.dense(self.gru, 1)\n",
    "        self.actor = tf.layers.dense(self.gru, num_actions)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1), nn.Linear(memsize, num_actions)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inputs, hx = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        hx = self.gru(x.view(-1, 32 * 5 * 5), (hx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx\n",
    "\n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; step = 0\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [int(s.split('.')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; step = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if step is 0 else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedAdam(torch.optim.Adam): # extend a pytorch optimizer so it shares grads across processes\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['shared_steps'], state['step'] = torch.zeros(1).share_memory_(), 0\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_().share_memory_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_().share_memory_()\n",
    "                \n",
    "        def step(self, closure=None):\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None: continue\n",
    "                    self.state[p]['shared_steps'] += 1\n",
    "                    self.state[p]['step'] = self.state[p]['shared_steps'][0] - 1 # a \"step += 1\"  comes later\n",
    "            super.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(args, values, logps, actions, rewards):\n",
    "    np_values = values.view(-1).data.numpy()\n",
    "\n",
    "    # generalized advantage estimation using \\delta_t residuals (a policy gradient method)\n",
    "    delta_t = np.asarray(rewards) + args.gamma * np_values[1:] - np_values[:-1]\n",
    "    logpys = logps.gather(1, torch.tensor(actions).view(-1,1))\n",
    "    gen_adv_est = discount(delta_t, args.gamma * args.tau)\n",
    "    policy_loss = -(logpys.view(-1) * torch.FloatTensor(gen_adv_est.copy())).sum()\n",
    "    \n",
    "    # l2 loss over value estimator\n",
    "    rewards[-1] += args.gamma * np_values[-1]\n",
    "    discounted_r = discount(np.asarray(rewards), args.gamma)\n",
    "    discounted_r = torch.tensor(discounted_r.copy(), dtype=torch.float32)\n",
    "    value_loss = .5 * (discounted_r - values[:-1,0]).pow(2).sum()\n",
    "\n",
    "    entropy_loss = -(-logps * torch.exp(logps)).sum() # encourage lower entropy\n",
    "    return policy_loss + 0.5 * value_loss + 0.01 * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(shared_model, shared_optimizer, rank, args, info):\n",
    "    env = gym.make(args.env) # make a local (unshared) environment\n",
    "    env.seed(args.seed + rank) ; torch.manual_seed(args.seed + rank) # seed everything\n",
    "    model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions) # a local/unshared model\n",
    "    state = torch.tensor(prepro(env.reset())) # get first state\n",
    "\n",
    "    start_time = last_disp_time = time.time()\n",
    "    episode_length, epr, eploss, done  = 0, 0, 0, True # bookkeeping\n",
    "\n",
    "    while info['frames'][0] <= 8e7 or args.test: # openai baselines uses 40M frames...we'll use 80M\n",
    "        model.load_state_dict(shared_model.state_dict()) # sync with shared model\n",
    "\n",
    "        hx = torch.zeros(1, 256) if done else hx.detach()  # rnn activation vector\n",
    "        values, logps, actions, rewards = [], [], [], [] # save values for computing gradientss\n",
    "\n",
    "        for step in range(args.rnn_steps):\n",
    "            episode_length += 1\n",
    "            value, logit, hx = model((state.view(1,1,80,80), hx))\n",
    "            logp = F.log_softmax(logit, dim=-1)\n",
    "\n",
    "            action = torch.exp(logp).multinomial(num_samples=1).data[0]#logp.max(1)[1].data if args.test else\n",
    "            state, reward, done, _ = env.step(action.numpy()[0])\n",
    "            if args.render: env.render()\n",
    "\n",
    "            state = torch.tensor(prepro(state)) ; epr += reward\n",
    "            reward = np.clip(reward, -1, 1) # reward\n",
    "            done = done or episode_length >= 1e4 # don't playing one ep for too long\n",
    "            \n",
    "            info['frames'].add_(1) ; num_frames = int(info['frames'].item())\n",
    "            if num_frames % 2e6 == 0: # save every 2M frames\n",
    "                printlog(args, '\\n\\t{:.0f}M frames: saved model\\n'.format(num_frames/1e6))\n",
    "                torch.save(shared_model.state_dict(), args.save_dir+'model.{:.0f}.tar'.format(num_frames/1e6))\n",
    "\n",
    "            if done: # update shared data\n",
    "                info['episodes'] += 1\n",
    "                interp = 1 if info['episodes'][0] == 1 else 1 - args.horizon\n",
    "                info['run_epr'].mul_(1-interp).add_(interp * epr)\n",
    "                info['run_loss'].mul_(1-interp).add_(interp * eploss)\n",
    "\n",
    "            if rank == 0 and time.time() - last_disp_time > 60: # print info ~ every minute\n",
    "                elapsed = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start_time))\n",
    "                printlog(args, 'time {}, episodes {:.0f}, frames {:.1f}M, mean epr {:.2f}, run loss {:.2f}'\n",
    "                    .format(elapsed, info['episodes'].item(), num_frames/1e6,\n",
    "                    info['run_epr'].item(), info['run_loss'].item()))\n",
    "                last_disp_time = time.time()\n",
    "\n",
    "            if done: # maybe print info.\n",
    "                episode_length, epr, eploss = 0, 0, 0\n",
    "                state = torch.tensor(prepro(env.reset()))\n",
    "\n",
    "            values.append(value) ; logps.append(logp) ; actions.append(action) ; rewards.append(reward)\n",
    "\n",
    "        next_value = torch.zeros(1,1) if done else model((state.unsqueeze(0), hx))[0]\n",
    "        values.append(next_value.detach())\n",
    "\n",
    "        loss = cost_func(args, torch.cat(values), torch.cat(logps), torch.cat(actions), np.asarray(rewards))\n",
    "        eploss += loss.item()\n",
    "        shared_optimizer.zero_grad() ; loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "\n",
    "        for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "            if shared_param.grad is None: shared_param._grad = param.grad # sync gradients with shared model\n",
    "        shared_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if sys.version_info[0] > 2:\n",
    "        mp.set_start_method('spawn') # this must not be in global scope\n",
    "    elif sys.platform == 'linux' or sys.platform == 'linux2':\n",
    "        raise \"Must be using Python 3 with linux!\" # or else you get a deadlock in conv2d\n",
    "    \n",
    "    args = get_args()\n",
    "    args.save_dir = '{}/'.format(args.env.lower()) # keep the directory structure simple\n",
    "    if args.render:  args.processes = 1 ; args.test = True # render mode -> test mode w one process\n",
    "    if args.test:  args.lr = 0 # don't train in render mode\n",
    "    args.num_actions = gym.make(args.env).action_space.n # get the action space of this game\n",
    "    os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc.\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    shared_model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions).share_memory()\n",
    "    shared_optimizer = SharedAdam(shared_model.parameters(), lr=args.lr)\n",
    "\n",
    "    info = {k: torch.DoubleTensor([0]).share_memory_() for k in ['run_epr', 'run_loss', 'episodes', 'frames']}\n",
    "    info['frames'] += shared_model.try_load(args.save_dir) * 1e6\n",
    "    if int(info['frames'].item()) == 0: printlog(args,'', end='', mode='w') # clear log file\n",
    "    \n",
    "    processes = []\n",
    "    for rank in range(args.processes):\n",
    "        p = mp.Process(target=train, args=(shared_model, shared_optimizer, rank, args, info))\n",
    "        p.start() ; processes.append(p)\n",
    "    for p in processes: p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
